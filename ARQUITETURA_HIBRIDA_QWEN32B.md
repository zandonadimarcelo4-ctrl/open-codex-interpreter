# ğŸ§  Arquitetura HÃ­brida: Qwen32B-MoE (CÃ©rebro) + DeepSeek-Lite (Executor)

## ğŸ¯ VisÃ£o Geral

Sistema hÃ­brido que combina:
- **Qwen2.5-32B-Instruct-MoE**: CÃ©rebro estratÃ©gico (raciocÃ­nio, planejamento, tool-calling)
- **DeepSeek-Coder-V2-Lite**: Executor rÃ¡pido (cÃ³digo, refatoraÃ§Ã£o, debugging)

**Resultado:** Sistema com inteligÃªncia tipo GPT-4-turbo + execuÃ§Ã£o local eficiente, tudo cabendo em 16GB VRAM.

---

## ğŸ“Š EspecificaÃ§Ãµes TÃ©cnicas

### Qwen2.5-32B-Instruct-MoE (CÃ©rebro)
- **Arquitetura:** MoE (Mixture of Experts)
- **VRAM:** ~12-14GB (Q4_K_M)
- **Especialistas ativos:** 2-4 por token (economia de VRAM)
- **InteligÃªncia:** ğŸ§  148 (similar a GPT-4-turbo)
- **Capacidades:**
  - âœ… RaciocÃ­nio profundo e estratÃ©gico
  - âœ… Planejamento multi-etapas
  - âœ… Tool calling nativo
  - âœ… Auto-reflexÃ£o e correÃ§Ã£o
  - âœ… ResoluÃ§Ã£o de problemas complexos

### DeepSeek-Coder-V2-Lite (Executor)
- **Arquitetura:** Dense (todos os pesos ativos)
- **VRAM:** ~8.5GB (Q4_K_M)
- **Velocidade:** ğŸš€ RÃ¡pida
- **InteligÃªncia:** ğŸ§  144 (excelente para cÃ³digo)
- **Capacidades:**
  - âœ… GeraÃ§Ã£o de cÃ³digo limpo
  - âœ… ExecuÃ§Ã£o precisa
  - âœ… Debugging e correÃ§Ã£o
  - âœ… RefatoraÃ§Ã£o e otimizaÃ§Ã£o
  - âœ… ExplicaÃ§Ã£o de cÃ³digo

---

## ğŸ—ï¸ Arquitetura do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AutoGen Commander (Orquestrador)           â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Qwen2.5-32B-Instruct-MoE (CÃ©rebro)             â”‚  â”‚
â”‚  â”‚  - RaciocÃ­nio estratÃ©gico                        â”‚  â”‚
â”‚  â”‚  - Planejamento multi-etapas                     â”‚  â”‚
â”‚  â”‚  - Tool calling                                  â”‚  â”‚
â”‚  â”‚  - Auto-reflexÃ£o                                 â”‚  â”‚
â”‚  â”‚  VRAM: ~12-14GB                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DeepSeek-Coder-V2-Lite (Executor)              â”‚  â”‚
â”‚  â”‚  - GeraÃ§Ã£o de cÃ³digo                             â”‚  â”‚
â”‚  â”‚  - ExecuÃ§Ã£o de cÃ³digo                            â”‚  â”‚
â”‚  â”‚  - Debugging                                     â”‚  â”‚
â”‚  â”‚  - RefatoraÃ§Ã£o                                   â”‚  â”‚
â”‚  â”‚  VRAM: ~8.5GB (carregado sob demanda)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Tools Locais                                    â”‚  â”‚
â”‚  â”‚  - Open Interpreter                              â”‚  â”‚
â”‚  â”‚  - After Effects MCP                             â”‚  â”‚
â”‚  â”‚  - Python/Shell/JavaScript                       â”‚  â”‚
â”‚  â”‚  - Browser-Use                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

### 1. VariÃ¡veis de Ambiente

```env
# Modelo principal (cÃ©rebro estratÃ©gico)
DEFAULT_MODEL=qwen2.5-32b-instruct-moe-rtx

# Modelo executor (cÃ³digo rÃ¡pido)
EXECUTOR_MODEL=deepseek-coder-v2-lite:instruct

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
```

### 2. CÃ³digo Python (AutoGen)

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient

# CÃ©rebro estratÃ©gico (Qwen32B-MoE)
brain_client = OpenAIChatCompletionClient(
    model="ollama/qwen2.5-32b-instruct-moe-rtx",
    api_base="http://localhost:11434/v1",
)

# Executor rÃ¡pido (DeepSeek-Lite)
executor_client = OpenAIChatCompletionClient(
    model="ollama/deepseek-coder-v2-lite:instruct",
    api_base="http://localhost:11434/v1",
)

# AutoGen Commander (usa brain_client)
commander = create_simple_commander(
    model="qwen2.5-32b-instruct-moe-rtx",
)

# Open Interpreter Agent (usa executor_client quando necessÃ¡rio)
interpreter_agent = create_open_interpreter_agent(
    model="deepseek-coder-v2-lite:instruct",
)
```

### 3. Roteamento Inteligente

```python
# Tarefas estratÃ©gicas â†’ Qwen32B-MoE
if task_requires_strategy or task_requires_planning:
    use_model = "qwen2.5-32b-instruct-moe-rtx"

# Tarefas de cÃ³digo â†’ DeepSeek-Lite
elif task_requires_code or task_requires_execution:
    use_model = "deepseek-coder-v2-lite:instruct"

# Tarefas hÃ­bridas â†’ Qwen32B-MoE (planeja) â†’ DeepSeek-Lite (executa)
else:
    # Qwen32B-MoE planeja
    plan = brain_client.generate_plan(task)
    # DeepSeek-Lite executa
    result = executor_client.execute_code(plan)
```

---

## ğŸ“ˆ BenefÃ­cios

### 1. InteligÃªncia MÃ¡xima
- âœ… RaciocÃ­nio tipo GPT-4-turbo (Qwen32B-MoE)
- âœ… Planejamento estratÃ©gico avanÃ§ado
- âœ… Auto-reflexÃ£o e correÃ§Ã£o

### 2. EficiÃªncia
- âœ… ExecuÃ§Ã£o rÃ¡pida (DeepSeek-Lite)
- âœ… Economia de VRAM (MoE ativa apenas especialistas necessÃ¡rios)
- âœ… Cache compartilhado (mesmo Ollama backend)

### 3. Flexibilidade
- âœ… Modelos podem ser trocados independentemente
- âœ… Roteamento inteligente baseado em tipo de tarefa
- âœ… Suporte a mÃºltiplos agentes simultÃ¢neos

---

## ğŸš€ InstalaÃ§Ã£o

### 1. Instalar Qwen32B-MoE
```bash
# Windows
scripts\setup_qwen32b_moe_rtx.bat

# Linux/macOS
ollama pull qwen2.5:32b
ollama create qwen2.5-32b-instruct-moe-rtx -f Modelfile.qwen2.5-32b-instruct-moe-rtx
```

### 2. Instalar DeepSeek-Lite
```bash
# Windows
scripts\setup_deepseek_lite_executor_rtx.bat

# Linux/macOS
ollama pull deepseek-coder-v2-lite:instruct
```

### 3. Configurar .env
```env
DEFAULT_MODEL=qwen2.5-32b-instruct-moe-rtx
EXECUTOR_MODEL=deepseek-coder-v2-lite:instruct
```

---

## ğŸ“Š ComparaÃ§Ã£o de Modelos

| Modelo | InteligÃªncia | VRAM | Tool Calling | Velocidade | Uso |
|--------|--------------|------|--------------|------------|-----|
| Qwen32B-MoE | ğŸ§  148 | ~13GB | âœ… Nativo | âš™ï¸ MÃ©dia | CÃ©rebro |
| DeepSeek-Lite | ğŸ§  144 | ~8.5GB | âš™ï¸ Manual | ğŸš€ RÃ¡pida | Executor |
| Qwen14B | ğŸ§  141 | ~9GB | âœ… Nativo | ğŸš€ RÃ¡pida | Alternativa |

---

## ğŸ¯ Casos de Uso

### 1. Planejamento EstratÃ©gico
- **Modelo:** Qwen32B-MoE
- **Uso:** Criar planos complexos, decompor tarefas, priorizar aÃ§Ãµes

### 2. ExecuÃ§Ã£o de CÃ³digo
- **Modelo:** DeepSeek-Lite
- **Uso:** Gerar cÃ³digo, executar, debugar, refatorar

### 3. Tool Calling
- **Modelo:** Qwen32B-MoE
- **Uso:** Chamar ferramentas, coordenar mÃºltiplas ferramentas

### 4. Auto-ReflexÃ£o
- **Modelo:** Qwen32B-MoE
- **Uso:** Refletir sobre aÃ§Ãµes, corrigir erros, melhorar planos

---

## ğŸ”§ OtimizaÃ§Ãµes

### 1. Cache Compartilhado
- âœ… Mesmo Ollama backend para ambos os modelos
- âœ… Cache de contexto compartilhado
- âœ… Economia de memÃ³ria

### 2. Roteamento Inteligente
- âœ… Detectar tipo de tarefa automaticamente
- âœ… Escolher modelo apropriado
- âœ… Balancear carga entre modelos

### 3. Load Balancing
- âœ… Carregar modelos sob demanda
- âœ… Descarregar modelos nÃ£o utilizados
- âœ… Manter apenas modelo ativo em VRAM

---

## âœ… ConclusÃ£o

**Arquitetura HÃ­brida = InteligÃªncia MÃ¡xima + EficiÃªncia MÃ¡xima**

- ğŸ§  **Qwen32B-MoE**: CÃ©rebro estratÃ©gico (raciocÃ­nio, planejamento, tool-calling)
- ğŸš€ **DeepSeek-Lite**: Executor rÃ¡pido (cÃ³digo, execuÃ§Ã£o, debugging)
- ğŸ’¾ **VRAM Total**: ~13GB (Qwen32B) + ~8.5GB (DeepSeek, sob demanda) = cabe em 16GB
- ğŸ¯ **Resultado**: Sistema com inteligÃªncia tipo GPT-4-turbo + execuÃ§Ã£o local eficiente

---

**Status:** âœ… Arquitetura hÃ­brida configurada e pronta para uso!

