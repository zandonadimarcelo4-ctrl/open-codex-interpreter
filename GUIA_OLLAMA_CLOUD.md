# ğŸ§  Guia Completo: Ollama Cloud + Local com Fallback

## ğŸ“Š VisÃ£o Geral

> **"Ollama Cloud como cÃ©rebro principal, modelos locais como fallback"**

Arquitetura hÃ­brida que combina:
- **Ollama Cloud** (cÃ©rebro principal) - modelos enormes (480B-671B), raciocÃ­nio profundo
- **Modelos locais** (fallback) - continuidade, offline, execuÃ§Ã£o rÃ¡pida
- **Fallback automÃ¡tico** - se Cloud falhar, usa Local automaticamente

---

## ğŸŒ Ollama Cloud: O que Ã©?

### **CaracterÃ­sticas**
- âœ… **Modelos enormes** (480B-671B) que nÃ£o cabem em GPUs pessoais
- âœ… **Hardware datacenter** - execuÃ§Ã£o rÃ¡pida em hardware profissional
- âœ… **Mesma API do Ollama** - compatÃ­vel com Ollama local
- âœ… **Privacidade** - Ollama nÃ£o retÃ©m ou registra suas consultas
- âœ… **Economia de bateria** - nÃ£o usa GPU local

### **Modelos DisponÃ­veis**
1. **qwen3-coder:480b-cloud** â­ **RECOMENDADO**
   - Especializado em cÃ³digo e agentes
   - Tool-calling nativo
   - Ideal para: AutomaÃ§Ã£o tÃ©cnica, execuÃ§Ã£o de ferramentas

2. **deepseek-v3.1:671b-cloud**
   - RaciocÃ­nio analÃ­tico profundo
   - Planejamento avanÃ§ado
   - Ideal para: Planejamento complexo, anÃ¡lise profunda

3. **gpt-oss:120b-cloud**
   - Modelo geral de alta qualidade
   - Boa para: Tarefas gerais, raciocÃ­nio

4. **kimi-k2:1t-cloud**
   - Modelo enorme (1T parÃ¢metros)
   - Ideal para: Tarefas extremamente complexas

5. **glm-4.6:cloud**
   - Modelo geral
   - Boa para: Tarefas gerais

6. **minimax-m2:cloud**
   - Modelo geral
   - Boa para: Tarefas gerais

---

## ğŸ’° Planos Ollama Cloud

| Plano | PreÃ§o | Uso | Ideal para |
|-------|-------|-----|------------|
| **Free** | $0 | Limitado | Testes, desenvolvimento |
| **Pro** | $20/mÃªs | Mais uso | Uso moderado |
| **Max** | $100/mÃªs | 5x mais que Pro | Uso intensivo |

**Limites:**
- Free: Limitado (horas/dia)
- Pro: Mais uso
- Max: 5x mais uso que Pro

**PrÃ³ximos:** Pricing baseado em uso (metered) em breve

---

## ğŸ”§ ConfiguraÃ§Ã£o

### **1. Criar Conta Ollama Cloud**
1. Acesse [https://ollama.com/cloud](https://ollama.com/cloud)
2. Crie uma conta (Free, Pro, ou Max)
3. FaÃ§a login: `ollama signin`

### **2. Configurar API Key (Opcional)**
```bash
# Gerar API key em https://ollama.com
export OLLAMA_API_KEY=your_api_key_here
```

### **3. Configurar VariÃ¡veis de Ambiente**
```env
# Ollama Cloud
OLLAMA_CLOUD_ENABLED=true
OLLAMA_CLOUD_MODEL=qwen3-coder:480b-cloud
OLLAMA_API_KEY=your_api_key_here
OLLAMA_CLOUD_BASE_URL=https://ollama.com

# Fallback automÃ¡tico
FALLBACK_ENABLED=true
```

### **4. Testar ConexÃ£o**
```bash
# Via CLI
ollama run qwen3-coder:480b-cloud "Hello, world!"

# Via API
curl https://ollama.com/api/tags \
  -H "Authorization: Bearer $OLLAMA_API_KEY"
```

---

## ğŸš€ Uso

### **Via CLI**
```bash
# Fazer login
ollama signin

# Executar modelo Cloud
ollama run qwen3-coder:480b-cloud "Planeje uma tarefa complexa"
```

### **Via API (Python)**
```python
import os
from ollama import Client

client = Client(
    host="https://ollama.com",
    headers={'Authorization': 'Bearer ' + os.environ.get('OLLAMA_API_KEY')}
)

messages = [
    {'role': 'user', 'content': 'Planeje uma tarefa complexa'},
]

for part in client.chat('qwen3-coder:480b-cloud', messages=messages, stream=True):
    print(part['message']['content'], end='', flush=True)
```

### **Via AutoGen (HybridCommander)**
```python
from super_agent.core.hybrid_commander import create_hybrid_commander

# Criar commander hÃ­brido
commander = create_hybrid_commander(
    cloud_model="qwen3-coder:480b-cloud",
    cloud_api_key=os.getenv("OLLAMA_API_KEY"),
    cloud_enabled=True,
    fallback_enabled=True,
)

# Processar mensagem (fallback automÃ¡tico)
response = await commander.process_message(
    "Planeje uma tarefa complexa e depois execute o cÃ³digo necessÃ¡rio"
)
```

---

## ğŸ”„ Fallback AutomÃ¡tico

### **Como Funciona**

1. **Tentativa Inicial (Cloud)**
   ```
   UsuÃ¡rio: "Planeje uma tarefa complexa"
   â†’ HybridCommander: Tenta Ollama Cloud
   â†’ Cloud disponÃ­vel? âœ…
   â†’ Resposta: Plano detalhado (Cloud)
   ```

2. **Fallback AutomÃ¡tico (Local)**
   ```
   UsuÃ¡rio: "Planeje uma tarefa complexa"
   â†’ HybridCommander: Tenta Ollama Cloud
   â†’ Cloud nÃ£o disponÃ­vel? âŒ (timeout, erro, quota)
   â†’ Fallback automÃ¡tico: Usa Ollama Local
   â†’ Resposta: Plano detalhado (Local)
   ```

3. **Fallback por Tipo de Tarefa**
   ```
   UsuÃ¡rio: "Execute cÃ³digo Python"
   â†’ HybridCommander: Detecta tipo de tarefa (execution)
   â†’ Roteia para Executor Local (mais rÃ¡pido)
   â†’ Resposta: CÃ³digo executado (Local)
   ```

---

## ğŸ“Š ComparaÃ§Ã£o: Cloud vs Local

| Aspecto | Ollama Cloud | Ollama Local |
|---------|--------------|--------------|
| **Modelos** | 480B-671B (enormes) | 32B (mÃ©dio) |
| **RaciocÃ­nio** | ğŸ§  Profundo | âš™ï¸ RazoÃ¡vel |
| **Velocidade** | âš ï¸ Mais lento (~10-25 t/s) | âš¡ RÃ¡pido (~50-100 t/s) |
| **Contexto** | âœ… Enorme (128K+ tokens) | âš™ï¸ MÃ©dio (32K tokens) |
| **Offline** | âŒ Requer internet | âœ… Totalmente offline |
| **Custo** | âš ï¸ Limitado (quota) | ğŸ’° Zero |
| **Privacidade** | âš ï¸ Dados na Cloud | âœ… Dados locais |
| **Disponibilidade** | âš ï¸ Dependente de serviÃ§o | âœ… Sempre disponÃ­vel |

---

## ğŸ¯ Quando Usar Cloud vs Local

### **Cloud (Ollama Cloud)**
- âœ… Planejamento complexo multi-etapas
- âœ… RaciocÃ­nio profundo e anÃ¡lise
- âœ… Contexto muito longo (128K+ tokens)
- âœ… Tarefas que requerem mÃ¡xima inteligÃªncia
- âœ… Quando GPU local nÃ£o Ã© suficiente

### **Local (Ollama Local)**
- âœ… ExecuÃ§Ã£o de cÃ³digo rÃ¡pida
- âœ… Tarefas simples e diretas
- âœ… Modo offline
- âœ… Privacidade mÃ¡xima
- âœ… Fallback quando Cloud nÃ£o disponÃ­vel
- âœ… Quando quota Cloud estÃ¡ esgotada

---

## ğŸ”§ BenefÃ­cios da Arquitetura HÃ­brida

### **1. InteligÃªncia MÃ¡xima**
- ğŸ§  **Ollama Cloud** fornece raciocÃ­nio profundo (480B-671B)
- âš™ï¸ **Modelos locais** fornecem continuidade e execuÃ§Ã£o rÃ¡pida

### **2. Continuidade Garantida**
- âœ… **Fallback automÃ¡tico** - se Cloud falhar, usa Local
- âœ… **Modo offline** - funciona sem internet
- âœ… **Nunca trava** - sempre tem fallback

### **3. Custo Otimizado**
- ğŸ’° **Cloud apenas para tarefas complexas** - economiza quota
- ğŸ’° **Local para tarefas simples** - zero custo
- ğŸ’° **Fallback inteligente** - usa Local quando possÃ­vel

### **4. Privacidade**
- ğŸ” **Dados sensÃ­veis** - usa Local (offline)
- ğŸ” **Dados nÃ£o sensÃ­veis** - usa Cloud (raciocÃ­nio profundo)
- ğŸ” **Controle total** - vocÃª decide quando usar Cloud

---

## ğŸ› Troubleshooting

### **Erro: "Cloud nÃ£o disponÃ­vel"**
```bash
# Verificar login
ollama signin

# Verificar conexÃ£o
curl https://ollama.com/api/tags \
  -H "Authorization: Bearer $OLLAMA_API_KEY"

# Verificar API key
echo $OLLAMA_API_KEY
```

### **Erro: "API key invÃ¡lida"**
```bash
# Gerar nova API key em https://ollama.com
export OLLAMA_API_KEY=your_new_api_key

# Atualizar .env
echo "OLLAMA_API_KEY=your_new_api_key" >> .env
```

### **Erro: "Quota esgotada"**
```bash
# Verificar plano Ollama Cloud
# Free: Limitado (horas/dia)
# Pro: Mais uso ($20/mÃªs)
# Max: 5x mais uso ($100/mÃªs)

# Fallback automÃ¡tico usa Local quando quota esgotada
```

### **Erro: "Timeout"**
```env
# Aumentar timeout
OLLAMA_CLOUD_TIMEOUT=60
OLLAMA_LOCAL_TIMEOUT=120
```

---

## âœ… ConclusÃ£o

**Arquitetura hÃ­brida = InteligÃªncia mÃ¡xima + Continuidade garantida**

- ğŸ§  **Ollama Cloud**: CÃ©rebro principal (raciocÃ­nio profundo, 480B-671B)
- âš™ï¸ **Ollama Local**: Fallback (continuidade, offline, execuÃ§Ã£o rÃ¡pida)
- ğŸ”„ **Fallback automÃ¡tico**: Nunca trava, sempre funciona
- ğŸ’° **Custo otimizado**: Cloud apenas para tarefas complexas
- ğŸ” **Privacidade**: Controle total sobre dados

---

## ğŸ“š ReferÃªncias

- [Ollama Cloud Documentation](https://docs.ollama.com/cloud)
- [Ollama Cloud Website](https://ollama.com/cloud)
- [ARQUITETURA_HIBRIDA_CLOUD_LOCAL.md](./ARQUITETURA_HIBRIDA_CLOUD_LOCAL.md)
- [THINKING_VS_INTELIGENCIA.md](./THINKING_VS_INTELIGENCIA.md)

---

**Status**: âœ… Guia completo, configuraÃ§Ã£o pronta, pronto para uso!

