# üß† An√°lise de Modelos Recomendados para Orquestra√ß√£o

## üìä Modelos Analisados

### 1. **UIGEN-T1-Qwen-14** (Especializado em UI)
- **Base**: Qwen2.5-Coder-14B-Instruct
- **Especializa√ß√£o**: Gera√ß√£o de UI com reasoning (HTML/CSS)
- **Tamanhos Dispon√≠veis**:
  - `q4_K_S`: 8.6GB (recomendado para 16GB VRAM)
  - `q6_K`: 12GB
  - `q8_0`: 16GB
  - `F16`: 30GB
- **VRAM Recomendado**: 12GB
- **Contexto**: 32K tokens
- **Uso Ideal**: Executor especializado para gera√ß√£o de UI/HTML/CSS

**Vantagens**:
- ‚úÖ Chain-of-thought reasoning para UI
- ‚úÖ Especializado em dashboards, landing pages, forms
- ‚úÖ Gera HTML/CSS estruturado e v√°lido
- ‚úÖ Vers√£o quantizada (q4_K_S) cabe em 16GB VRAM

**Desvantagens**:
- ‚ö†Ô∏è Limitado a UI b√°sica (n√£o JavaScript pesado)
- ‚ö†Ô∏è Pode ter artifacts de formata√ß√£o
- ‚ö†Ô∏è Design repetitivo (treinado em dataset limitado)

**Recomenda√ß√£o**: ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) - Excelente como executor especializado para UI

---

### 2. **deepseek-coder-v2-lite-base-q4_k_m-gguf** (Executor Quantizado)
- **Base**: DeepSeek-Coder-V2-Lite
- **Quantiza√ß√£o**: Q4_K_M (balanceada)
- **Especializa√ß√£o**: Gera√ß√£o de c√≥digo multi-linguagem
- **Tamanho Estimado**: ~6-8GB (Q4_K_M)
- **Uso Ideal**: Executor de c√≥digo geral (Python, JavaScript, etc.)

**Vantagens**:
- ‚úÖ Quantiza√ß√£o Q4_K_M (boa qualidade/mem√≥ria)
- ‚úÖ Baseada em DeepSeek-Coder-V2-Lite (c√≥digo testado)
- ‚úÖ Suporte a 338 linguagens
- ‚úÖ Eficiente para VRAM limitada

**Desvantagens**:
- ‚ö†Ô∏è Quantiza√ß√£o pode reduzir qualidade levemente
- ‚ö†Ô∏è Menos especializado que UIGEN para UI

**Recomenda√ß√£o**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5) - Melhor executor geral quantizado

---

### 3. **zhi-create-qwen3-32b** (Brain Criativo)
- **Base**: Qwen3 32B
- **Especializa√ß√£o**: Cria√ß√£o/conte√∫do criativo
- **Tamanho Estimado**: ~20-30GB (dependendo da quantiza√ß√£o)
- **Uso Ideal**: Brain estrat√©gico para tarefas criativas

**Vantagens**:
- ‚úÖ Modelo 32B (mais inteligente que 14B)
- ‚úÖ Especializado em cria√ß√£o/conte√∫do
- ‚úÖ Potencialmente mais criativo

**Desvantagens**:
- ‚ö†Ô∏è Tamanho grande (pode n√£o caber em 16GB VRAM mesmo quantizado)
- ‚ö†Ô∏è Menos testado que Qwen2.5-32B-MoE
- ‚ö†Ô∏è Pode ser menos eficiente que MoE

**Recomenda√ß√£o**: ‚≠ê‚≠ê‚≠ê (3/5) - Interessante, mas precisa verificar tamanho/quantiza√ß√£o

---

## üéØ Recomenda√ß√µes por Caso de Uso

### **Caso 1: Executor de C√≥digo Geral**
**Recomenda√ß√£o**: `deepseek-coder-v2-lite-base-q4_k_m-gguf`
- ‚úÖ Mais eficiente (quantiza√ß√£o Q4_K_M)
- ‚úÖ Suporte amplo a linguagens
- ‚úÖ Cabe em 16GB VRAM com Brain

### **Caso 2: Executor Especializado em UI**
**Recomenda√ß√£o**: `UIGEN-T1-Qwen-14:q4_K_S`
- ‚úÖ Especializado em HTML/CSS
- ‚úÖ Chain-of-thought reasoning
- ‚úÖ 8.6GB (cabe em 16GB VRAM)

### **Caso 3: Brain Estrat√©gico**
**Recomenda√ß√£o**: `qwen2.5-32b-instruct-moe-rtx` (atual) ou `zhi-create-qwen3-32b` (se couber)
- ‚úÖ Qwen2.5-32B-MoE: Testado, eficiente (MoE), cabe em 16GB
- ‚úÖ Qwen3-32B: Mais criativo, mas precisa verificar tamanho

---

## üîß Configura√ß√£o Recomendada (16GB VRAM)

### **Op√ß√£o 1: Geral (Recomendada)**
```env
# Brain: Estrat√©gico (fixo)
DEFAULT_MODEL=qwen2.5-32b-instruct-moe-rtx

# Executor: C√≥digo geral (carregado sob demanda)
EXECUTOR_MODEL=networkjohnny/deepseek-coder-v2-lite-base-q4_k_m-gguf
```

**VRAM Esperada**:
- Brain: ~13GB
- Executor: ~6-8GB
- **Total**: Nunca estoura 16GB (modo alternado)

### **Op√ß√£o 2: UI Especializado**
```env
# Brain: Estrat√©gico (fixo)
DEFAULT_MODEL=qwen2.5-32b-instruct-moe-rtx

# Executor: UI especializado (carregado sob demanda)
EXECUTOR_MODEL=MHKetbi/UIGEN-T1-Qwen-14:q4_K_S
```

**VRAM Esperada**:
- Brain: ~13GB
- Executor: ~8.6GB
- **Total**: Nunca estoura 16GB (modo alternado)

### **Op√ß√£o 3: Criativo (Experimental)**
```env
# Brain: Criativo (fixo) - PRECISA VERIFICAR TAMANHO
DEFAULT_MODEL=zhihu/zhi-create-qwen3-32b

# Executor: C√≥digo geral (carregado sob demanda)
EXECUTOR_MODEL=networkjohnny/deepseek-coder-v2-lite-base-q4_k_m-gguf
```

**VRAM Esperada**:
- Brain: ~20-30GB? (PRECISA VERIFICAR)
- Executor: ~6-8GB
- **‚ö†Ô∏è PODE ESTOURAR 16GB** (n√£o recomendado sem quantiza√ß√£o)

---

## üì• Instala√ß√£o dos Modelos

### **1. DeepSeek-Coder-V2-Lite (Q4_K_M)**
```bash
# Pull do modelo quantizado
ollama pull networkjohnny/deepseek-coder-v2-lite-base-q4_k_m-gguf

# Verificar instala√ß√£o
ollama list | grep deepseek-coder-v2-lite
```

### **2. UIGEN-T1-Qwen-14 (Q4_K_S)**
```bash
# Pull do modelo quantizado
ollama pull MHKetbi/UIGEN-T1-Qwen-14:q4_K_S

# Verificar instala√ß√£o
ollama list | grep UIGEN-T1-Qwen-14
```

### **3. Zhi-Create-Qwen3-32B (Experimental)**
```bash
# Pull do modelo (verificar tamanho primeiro)
ollama pull zhihu/zhi-create-qwen3-32b

# Verificar tamanho
ollama show zhihu/zhi-create-qwen3-32b
```

---

## üéØ Decis√£o Final

### **Recomenda√ß√£o Principal**: 
**Usar `deepseek-coder-v2-lite-base-q4_k_m-gguf` como executor padr√£o**

**Raz√µes**:
1. ‚úÖ Quantiza√ß√£o Q4_K_M (boa qualidade/mem√≥ria)
2. ‚úÖ Mais eficiente que UIGEN para c√≥digo geral
3. ‚úÖ Suporte amplo a linguagens (338)
4. ‚úÖ Cabe em 16GB VRAM com Brain
5. ‚úÖ Baseada em modelo testado (DeepSeek-Coder-V2-Lite)

### **Recomenda√ß√£o Secund√°ria**:
**Usar `UIGEN-T1-Qwen-14:q4_K_S` como executor alternativo para UI**

**Raz√µes**:
1. ‚úÖ Especializado em HTML/CSS
2. ‚úÖ Chain-of-thought reasoning
3. ‚úÖ 8.6GB (cabe em 16GB VRAM)
4. ‚úÖ √ötil para tarefas de UI espec√≠ficas

### **Recomenda√ß√£o Terci√°ria**:
**Manter `qwen2.5-32b-instruct-moe-rtx` como brain padr√£o**

**Raz√µes**:
1. ‚úÖ Testado e otimizado para RTX
2. ‚úÖ MoE (eficiente)
3. ‚úÖ Cabe em 16GB VRAM
4. ‚úÖ Mais confi√°vel que Qwen3-32B (menos testado)

---

## üîÑ Configura√ß√£o no C√≥digo

### **Atualizar `env.example`**
```env
# Brain: Estrat√©gico (fixo)
DEFAULT_MODEL=qwen2.5-32b-instruct-moe-rtx

# Executor: C√≥digo geral (carregado sob demanda)
EXECUTOR_MODEL=networkjohnny/deepseek-coder-v2-lite-base-q4_k_m-gguf

# Executor alternativo: UI especializado (opcional)
EXECUTOR_UI_MODEL=MHKetbi/UIGEN-T1-Qwen-14:q4_K_S
```

### **Atualizar `model_manager.py`**
```python
# Suportar m√∫ltiplos executores
executor_model = os.getenv("EXECUTOR_MODEL", "networkjohnny/deepseek-coder-v2-lite-base-q4_k_m-gguf")
executor_ui_model = os.getenv("EXECUTOR_UI_MODEL", "MHKetbi/UIGEN-T1-Qwen-14:q4_K_S")
```

### **Atualizar `intelligent_router.py`**
```python
# Detectar tarefas de UI e usar executor UI especializado
if task_type == TaskType.UI_GENERATION:
    return executor_ui_model
else:
    return executor_model
```

---

## ‚úÖ Pr√≥ximos Passos

1. ‚úÖ **Instalar modelos recomendados**
2. ‚úÖ **Atualizar configura√ß√£o (env.example)**
3. ‚úÖ **Atualizar model_manager.py para suportar m√∫ltiplos executores**
4. ‚úÖ **Atualizar intelligent_router.py para detectar tarefas de UI**
5. ‚úÖ **Testar orquestra√ß√£o com modelos novos**
6. ‚úÖ **Verificar VRAM usage (nunca estourar 16GB)**

---

**Status**: ‚úÖ An√°lise completa, recomenda√ß√µes definidas, pronto para implementa√ß√£o!

