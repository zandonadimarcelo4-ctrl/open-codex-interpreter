# Guia de Configura√ß√£o - Open Codex Interpreter

## üöÄ Setup R√°pido (Windows)

### 1. Executar Setup Autom√°tico
```batch
setup_windows.bat
```

Este script ir√°:
- ‚úÖ Verificar Python
- ‚úÖ Criar ambiente virtual
- ‚úÖ Instalar depend√™ncias
- ‚úÖ Criar arquivo `.env`
- ‚úÖ Criar diret√≥rios necess√°rios

### 2. Configurar Vari√°veis de Ambiente (Opcional)
Edite o arquivo `.env` criado e configure:
- `OLLAMA_BASE_URL` - URL do Ollama (padr√£o: http://localhost:11434)
- `DATABASE_URL` - URL do banco de dados (padr√£o: SQLite)
- `PORT` - Porta do servidor (padr√£o: 8080)

### 3. Iniciar Servidor
```batch
start_windows.bat
```

Ou:
```batch
python -m open_codex serve
```

### 4. Acessar Interface Web
Abra no navegador:
```
http://localhost:8080
```

## üìã Setup Manual

### Pr√©-requisitos
- Python 3.10 ou superior
- pip (geralmente vem com Python)

### Passo 1: Criar Ambiente Virtual
```bash
python -m venv .venv
```

### Passo 2: Ativar Ambiente Virtual

**Windows:**
```batch
.venv\Scripts\activate
```

**Linux/Mac:**
```bash
source .venv/bin/activate
```

### Passo 3: Instalar Depend√™ncias
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Passo 4: Configurar Vari√°veis de Ambiente
```bash
# Copiar arquivo de exemplo
cp .env.example .env

# Editar .env com suas configura√ß√µes
# (opcional, valores padr√£o funcionam)
```

### Passo 5: Criar Diret√≥rios
```bash
mkdir -p data/uploads
mkdir -p data/cache
```

### Passo 6: Iniciar Servidor

**Op√ß√£o 1: Usando script**
```batch
# Windows
start_windows.bat

# Linux/Mac
./start.sh
```

**Op√ß√£o 2: Usando Python**
```bash
python -m open_codex serve
```

**Op√ß√£o 3: Usando uvicorn diretamente**
```bash
uvicorn open_webui.main:app --host 0.0.0.0 --port 8080 --reload
```

## üîß Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente Importantes

#### Seguran√ßa
```bash
WEBUI_SECRET_KEY=          # Gerado automaticamente se n√£o fornecido
WEBUI_JWT_SECRET_KEY=      # Gerado automaticamente se n√£o fornecido
```

#### Servidor
```bash
HOST=0.0.0.0              # Host do servidor
PORT=8080                  # Porta do servidor
UVICORN_WORKERS=1          # N√∫mero de workers
```

#### Banco de Dados
```bash
# SQLite (desenvolvimento)
DATABASE_URL=sqlite:///./data/webui.db

# PostgreSQL (produ√ß√£o)
DATABASE_URL=postgresql://user:password@localhost:5432/open_codex

# MySQL (produ√ß√£o)
DATABASE_URL=mysql://user:password@localhost:3306/open_codex
```

#### Ollama (Modelos Locais)
```bash
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_API_BASE_URL=http://localhost:11434/api
ENABLE_OLLAMA_API=True
```

#### Open Interpreter
```bash
OPENINTERPRETER_MODE=local
OPENINTERPRETER_SANDBOX=false
```

### Instalar Ollama (Opcional)

Para usar modelos locais, instale o Ollama:

**Windows:**
1. Baixe de: https://ollama.ai/download
2. Instale e execute
3. Baixe um modelo: `ollama pull llama2`

**Linux/Mac:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama2
```

## üß™ Testar Instala√ß√£o

### 1. Verificar Python
```bash
python --version
# Deve mostrar Python 3.10 ou superior
```

### 2. Verificar Depend√™ncias
```bash
python -c "import fastapi; print('FastAPI OK')"
python -c "import uvicorn; print('Uvicorn OK')"
python -c "import open_webui; print('Open WebUI OK')"
```

### 3. Testar Servidor
```bash
python -m open_codex serve
```

Acesse: http://localhost:8080

## üêõ Solu√ß√£o de Problemas

### Erro: "ModuleNotFoundError: No module named 'open_webui'"
**Solu√ß√£o:** Certifique-se de que o ambiente virtual est√° ativado e as depend√™ncias foram instaladas.

### Erro: "Port already in use"
**Solu√ß√£o:** Altere a porta no `.env` ou pare o processo que est√° usando a porta 8080.

### Erro: "Database connection failed"
**Solu√ß√£o:** Verifique a `DATABASE_URL` no `.env`. Para SQLite, certifique-se de que o diret√≥rio `data/` existe.

### Erro: "Ollama connection failed"
**Solu√ß√£o:** 
- Verifique se o Ollama est√° rodando: `ollama serve`
- Verifique a `OLLAMA_BASE_URL` no `.env`
- Ou desabilite: `ENABLE_OLLAMA_API=False`

## üìù Pr√≥ximos Passos

1. **Acessar Interface Web**: http://localhost:8080
2. **Criar Primeiro Usu√°rio**: A interface pedir√° para criar um usu√°rio admin
3. **Configurar Modelos**: Adicione modelos Ollama ou OpenAI
4. **Testar Chat**: Comece a usar o chat com o modelo configurado

## üîó Links √öteis

- **Ollama**: https://ollama.ai
- **Open Interpreter**: https://github.com/KillianLucas/open-interpreter
- **Open WebUI**: https://github.com/open-webui/open-webui
- **Documenta√ß√£o FastAPI**: https://fastapi.tiangolo.com

## üí° Dicas

1. **Desenvolvimento**: Use `--reload` para auto-reload:
   ```bash
   uvicorn open_webui.main:app --reload
   ```

2. **Produ√ß√£o**: Use m√∫ltiplos workers:
   ```bash
   UVICORN_WORKERS=4 python -m open_codex serve
   ```

3. **Logs**: Configure `GLOBAL_LOG_LEVEL=DEBUG` no `.env` para mais detalhes

4. **Banco de Dados**: SQLite √© suficiente para desenvolvimento. Use PostgreSQL/MySQL para produ√ß√£o.

---

**Pronto!** Agora voc√™ pode testar a interface web. üéâ

