# ğŸ§  OrquestraÃ§Ã£o Inteligente de Modelos (16GB VRAM)

## ğŸ¯ Objetivo

Gerenciar dois modelos na GPU de 16GB usando **modo alternado**:
- **Qwen2.5-32B-MoE**: CÃ©rebro estratÃ©gico (fixo, ~13GB VRAM)
- **Qwen2.5-Coder-14B**: Executor de cÃ³digo (carregado sob demanda, ~9GB VRAM)

**Resultado:** Sistema com inteligÃªncia mÃ¡xima + execuÃ§Ã£o eficiente, sem estourar VRAM.

---

## âš™ï¸ EstratÃ©gia: Modo Alternado

### Como Funciona

1. **Brain (Qwen32B-MoE)** fica carregado na GPU (~13GB VRAM)
2. **Executor (Qwen14B-Coder)** Ã© carregado sob demanda quando necessÃ¡rio
3. Quando Executor Ã© carregado, Brain Ã© descarregado automaticamente (liberando VRAM)
4. ApÃ³s Executor terminar, Brain Ã© recarregado automaticamente

### Vantagens

- âœ… **Nunca estoura VRAM** (apenas um modelo por vez)
- âœ… **100% GPU** (ambos modelos rodam totalmente na GPU)
- âœ… **AlternÃ¢ncia automÃ¡tica** (sem intervenÃ§Ã£o manual)
- âœ… **Baixa latÃªncia** (troca de modelo ~1-2s)

### Desvantagens

- âš ï¸ **Pequena pausa** na troca de modelo (~1-2s)
- âš ï¸ **NÃ£o pode usar ambos simultaneamente** (mas nÃ£o Ã© necessÃ¡rio)

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SmartCommander (Orquestrador)                â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  IntelligentRouter                              â”‚  â”‚
â”‚  â”‚  - Detecta tipo de tarefa                       â”‚  â”‚
â”‚  â”‚  - Roteia para modelo apropriado                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ModelManager                                    â”‚  â”‚
â”‚  â”‚  - Gerencia carregamento/descarregamento        â”‚  â”‚
â”‚  â”‚  - Alterna modelos automaticamente              â”‚  â”‚
â”‚  â”‚  - Monitora uso de VRAM                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Qwen2.5-32B-MoE (Brain)                        â”‚  â”‚
â”‚  â”‚  - RaciocÃ­nio estratÃ©gico                       â”‚  â”‚
â”‚  â”‚  - Planejamento multi-etapas                    â”‚  â”‚
â”‚  â”‚  - Tool calling                                 â”‚  â”‚
â”‚  â”‚  VRAM: ~13GB (fixo)                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â†“                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Qwen2.5-Coder-14B (Executor)                   â”‚  â”‚
â”‚  â”‚  - GeraÃ§Ã£o de cÃ³digo                            â”‚  â”‚
â”‚  â”‚  - ExecuÃ§Ã£o de cÃ³digo                           â”‚  â”‚
â”‚  â”‚  - Debugging                                    â”‚  â”‚
â”‚  â”‚  VRAM: ~9GB (carregado sob demanda)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š DetecÃ§Ã£o de Tarefas

### Tarefas â†’ Brain (Qwen32B-MoE)
- âœ… Planejamento estratÃ©gico
- âœ… RaciocÃ­nio complexo
- âœ… Tool calling
- âœ… Auto-reflexÃ£o
- âœ… Conversas

### Tarefas â†’ Executor (Qwen14B-Coder)
- âœ… GeraÃ§Ã£o de cÃ³digo
- âœ… ExecuÃ§Ã£o de cÃ³digo
- âœ… Debugging
- âœ… RefatoraÃ§Ã£o

---

## ğŸš€ Uso

### CÃ³digo Python

```python
from super_agent.core.smart_commander import create_smart_commander

# Criar commander inteligente
commander = create_smart_commander(
    brain_model="qwen2.5-32b-instruct-moe-rtx",
    executor_model="qwen2.5-coder:14b",
)

# Processar mensagem (rota automaticamente)
response = await commander.process_message(
    "Planeje uma tarefa complexa e depois execute o cÃ³digo necessÃ¡rio"
)

# Ver status
status = commander.get_status()
print(f"Modelo atual: {status['current_model']}")
print(f"VRAM usada: {status['vram_used_gb']:.2f}GB")
```

### DetecÃ§Ã£o AutomÃ¡tica

```python
# Tarefa estratÃ©gica â†’ Brain
response = await commander.process_message("Analise e planeje uma soluÃ§Ã£o")

# Tarefa de cÃ³digo â†’ Executor
response = await commander.process_message("Crie um script Python que soma nÃºmeros")

# Tarefa hÃ­brida â†’ Brain planeja, Executor executa
response = await commander.process_message(
    "Planeje e execute uma tarefa complexa"
)
```

---

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```env
# Modelo cÃ©rebro estratÃ©gico
DEFAULT_MODEL=qwen2.5-32b-instruct-moe-rtx

# Modelo executor de cÃ³digo
EXECUTOR_MODEL=qwen2.5-coder:14b

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
```

### InstalaÃ§Ã£o de Modelos

```bash
# Instalar Brain (Qwen32B-MoE)
scripts\setup_qwen32b_moe_rtx.bat

# Instalar Executor (Qwen14B-Coder)
ollama pull qwen2.5-coder:14b
```

---

## ğŸ“ˆ Monitoramento

### Status do Gerenciador

```python
from super_agent.core.model_manager import get_model_manager

manager = get_model_manager()
status = manager.get_status()

print(f"Modelo atual: {status['current_model']}")
print(f"Papel: {status['current_role']}")
print(f"VRAM usada: {status['vram_used_gb']:.2f}GB / {status['vram_total_gb']:.2f}GB")
print(f"Brain carregado: {status['brain_loaded']}")
print(f"Executor carregado: {status['executor_loaded']}")
```

### Verificar VRAM

```bash
# Windows
nvidia-smi

# Linux
nvidia-smi
```

**VRAM esperada:**
- Brain carregado: ~13GB
- Executor carregado: ~9GB
- **Nunca estoura 16GB** (modo alternado)

---

## ğŸ¯ Casos de Uso

### 1. Tarefa EstratÃ©gica
```
UsuÃ¡rio: "Analise e planeje uma soluÃ§Ã£o para X"
â†’ Router detecta: PLANNING
â†’ Usa Brain (Qwen32B-MoE)
â†’ Resposta: Plano detalhado
```

### 2. Tarefa de CÃ³digo
```
UsuÃ¡rio: "Crie um script Python que faz X"
â†’ Router detecta: CODE
â†’ Alterna para Executor (Qwen14B-Coder)
â†’ Resposta: CÃ³digo gerado e executado
```

### 3. Tarefa HÃ­brida
```
UsuÃ¡rio: "Planeje e execute uma tarefa complexa"
â†’ Router detecta: PLANNING
â†’ Usa Brain para planejar
â†’ Router detecta: EXECUTION
â†’ Alterna para Executor para executar
â†’ Resposta: Tarefa planejada e executada
```

---

## ğŸ› Troubleshooting

### Erro: "out of memory"
```bash
# Verificar VRAM
nvidia-smi

# Se estourar, verificar se ambos modelos estÃ£o carregados
# O gerenciador deve alternar automaticamente
```

### Modelo nÃ£o alterna
```bash
# Verificar se gerenciador estÃ¡ ativo
python -c "from super_agent.core.model_manager import get_model_manager; print(get_model_manager().get_status())"
```

### LatÃªncia alta na troca
```bash
# Verificar se Ollama estÃ¡ otimizado
ollama --version

# Verificar se modelos estÃ£o instalados
ollama list
```

---

## âœ… ConclusÃ£o

**OrquestraÃ§Ã£o Inteligente = InteligÃªncia MÃ¡xima + EficiÃªncia MÃ¡xima**

- ğŸ§  **Brain (Qwen32B-MoE)**: CÃ©rebro estratÃ©gico fixo
- ğŸš€ **Executor (Qwen14B-Coder)**: Executor carregado sob demanda
- ğŸ’¾ **VRAM Total**: ~13GB (Brain) ou ~9GB (Executor) = nunca estoura 16GB
- ğŸ¯ **Resultado**: Sistema com inteligÃªncia tipo GPT-4-turbo + execuÃ§Ã£o local eficiente

---

**Status:** âœ… OrquestraÃ§Ã£o inteligente implementada e pronta para uso!

