# üöÄ Guia: Quantiza√ß√£o do DeepSeek-Coder-V2:16b

## üìã Vis√£o Geral

Este guia mostra como usar o modelo **DeepSeek-Coder-V2:16b** oficial do Ollama e como quantiz√°-lo para vers√µes ainda mais leves.

---

## üéØ Op√ß√µes Dispon√≠veis

### 1. **Modelo Oficial do Ollama (Recomendado)**

O modelo oficial `deepseek-coder-v2:16b` j√° vem quantizado pelo Ollama:
- **Tamanho**: 8.9GB
- **Context Window**: 160K tokens
- **Quantiza√ß√£o**: Autom√°tica (otimizada pelo Ollama)

**Uso:**
```bash
# Baixar modelo oficial
ollama pull deepseek-coder-v2:16b

# Usar Modelfile otimizado
ollama create deepseek-coder-v2-16b-optimized -f Modelfile.deepseek-coder-v2-16b

# Testar
ollama run deepseek-coder-v2-16b-optimized "Write a Python function to calculate fibonacci"
```

---

### 2. **Quantiza√ß√£o Manual (Vers√µes Mais Leves)**

Se voc√™ precisa de vers√µes ainda mais leves, pode quantizar manualmente usando `llama.cpp`:

#### Op√ß√µes de Quantiza√ß√£o:

| Quantiza√ß√£o | Tamanho | Qualidade | VRAM | Uso Recomendado |
|-------------|---------|-----------|------|-----------------|
| **Q4_K_M** | ~5GB | 95% | ~6GB | **Recomendado** (melhor equil√≠brio) |
| **Q3_K_M** | ~4GB | 90% | ~5GB | Sistemas com mem√≥ria limitada |
| **Q2_K** | ~3GB | 85% | ~4GB | Sistemas muito limitados |

---

## üîß M√©todo 1: Usar Modelo Oficial (Mais F√°cil)

### Passo 1: Baixar Modelo

```bash
ollama pull deepseek-coder-v2:16b
```

### Passo 2: Criar Modelfile Otimizado

```bash
# Usar Modelfile pr√©-configurado
ollama create deepseek-coder-v2-16b-optimized -f Modelfile.deepseek-coder-v2-16b
```

### Passo 3: Testar

```bash
ollama run deepseek-coder-v2-16b-optimized "Write a Python function to reverse a string"
```

---

## üîß M√©todo 2: Quantiza√ß√£o Manual (Vers√µes Mais Leves)

### Pr√©-requisitos

1. **Ollama instalado**
2. **llama.cpp compilado**
3. **Modelo baixado do Ollama**

### Passo 1: Instalar Depend√™ncias

```bash
# Clonar llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Compilar (Linux/macOS)
make

# Compilar (Windows)
cmake -B build
cmake --build build --config Release
```

### Passo 2: Localizar Modelo do Ollama

O modelo do Ollama est√° em:
- **Linux/macOS**: `~/.ollama/models/`
- **Windows**: `C:\Users\<user>\.ollama\models\`

```bash
# Encontrar arquivo do modelo
find ~/.ollama/models -name "*deepseek-coder-v2*16b*" -type f
```

### Passo 3: Quantizar Manualmente

```bash
# Quantizar para Q4_K_M (recomendado)
./llama.cpp/quantize \
  ~/.ollama/models/.../model.gguf \
  ./models/deepseek-coder-v2-16b-q4_k_m.gguf \
  Q4_K_M

# Quantizar para Q3_K_M (mais leve)
./llama.cpp/quantize \
  ~/.ollama/models/.../model.gguf \
  ./models/deepseek-coder-v2-16b-q3_k_m.gguf \
  Q3_K_M

# Quantizar para Q2_K (muito leve)
./llama.cpp/quantize \
  ~/.ollama/models/.../model.gguf \
  ./models/deepseek-coder-v2-16b-q2_k.gguf \
  Q2_K
```

### Passo 4: Criar Modelfile para Quantiza√ß√£o

```bash
# Criar Modelfile para Q4_K_M
cat > Modelfile.deepseek-coder-v2-16b-q4_k_m << 'EOF'
FROM ./models/deepseek-coder-v2-16b-q4_k_m.gguf

SYSTEM """You are DeepSeek Coder V2, an expert AI assistant specialized in programming and code generation."""

TEMPLATE """<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|><|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"""

PARAMETER temperature 0.2
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx 16384
PARAMETER num_predict 4096
PARAMETER repeat_penalty 1.1
PARAMETER repeat_last_n 64
PARAMETER num_thread -1
PARAMETER num_batch 512
PARAMETER num_gpu -1
PARAMETER stop "<|start_header_id|>"
PARAMETER stop "<|end_header_id|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "```"
PARAMETER penalize_newline false
EOF
```

### Passo 5: Criar Modelo no Ollama

```bash
ollama create deepseek-coder-v2-16b-q4_k_m -f Modelfile.deepseek-coder-v2-16b-q4_k_m
```

---

## üöÄ M√©todo 3: Script Automatizado (Recomendado)

### Linux/macOS

```bash
# Tornar script execut√°vel
chmod +x scripts/quantize_deepseek_coder_v2.sh

# Executar
./scripts/quantize_deepseek_coder_v2.sh
```

### Windows

```powershell
# Executar script Python
python scripts/quantize_deepseek_coder_v2.py
```

O script ir√°:
1. ‚úÖ Verificar se Ollama est√° instalado
2. ‚úÖ Baixar modelo se necess√°rio
3. ‚úÖ Localizar arquivo do modelo
4. ‚úÖ Compilar llama.cpp se necess√°rio
5. ‚úÖ Quantizar para Q4_K_M, Q3_K_M e Q2_K
6. ‚úÖ Criar Modelfiles para cada quantiza√ß√£o

---

## üìä Compara√ß√£o de Quantiza√ß√µes

| M√©trica | Original (Ollama) | Q4_K_M | Q3_K_M | Q2_K |
|---------|-------------------|--------|--------|------|
| **Tamanho** | 8.9GB | ~5GB | ~4GB | ~3GB |
| **VRAM** | ~10GB | ~6GB | ~5GB | ~4GB |
| **Qualidade** | 100% | 95% | 90% | 85% |
| **Velocidade** | 100% | 110% | 120% | 130% |
| **Context** | 160K | 16384 | 16384 | 16384 |

**Recomenda√ß√£o**: Use **Q4_K_M** para o melhor equil√≠brio entre qualidade e tamanho.

---

## üîß Configura√ß√£o no Projeto ANIMA

### 1. Atualizar `.env`

```bash
# Usar modelo oficial otimizado
DEFAULT_MODEL=deepseek-coder-v2-16b-optimized

# Ou usar quantiza√ß√£o manual
# DEFAULT_MODEL=deepseek-coder-v2-16b-q4_k_m
```

### 2. Atualizar `ollama.ts`

O sistema j√° est√° configurado para priorizar modelos n√£o quantizados e fazer fallback autom√°tico.

### 3. Testar

```bash
# Testar modelo oficial
ollama run deepseek-coder-v2-16b-optimized "Write a Python function"

# Testar quantiza√ß√£o
ollama run deepseek-coder-v2-16b-q4_k_m "Write a Python function"
```

---

## üêõ Troubleshooting

### Erro: "Model not found"

```bash
# Verificar modelos instalados
ollama list

# Baixar modelo
ollama pull deepseek-coder-v2:16b
```

### Erro: "llama.cpp not found"

```bash
# Clonar e compilar llama.cpp
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make  # Linux/macOS
# ou
cmake -B build && cmake --build build --config Release  # Windows
```

### Erro: "Out of memory"

```bash
# Usar quantiza√ß√£o mais leve (Q3_K_M ou Q2_K)
# Ou reduzir num_ctx no Modelfile
PARAMETER num_ctx 8192  # Reduzir de 16384 para 8192
```

### Erro: "Quantization failed"

```bash
# Verificar se modelo est√° no formato GGUF
# Verificar se llama.cpp est√° compilado corretamente
# Tentar quantiza√ß√£o mais simples (Q4_K_M primeiro)
```

---

## üìö Refer√™ncias

- [Ollama DeepSeek-Coder-V2](https://ollama.com/library/deepseek-coder-v2)
- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [GGUF Quantization](https://github.com/ggerganov/llama.cpp/blob/master/docs/QUANTIZATION.md)
- [DeepSeek-Coder-V2 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)

---

## ‚úÖ Checklist

- [ ] Ollama instalado
- [ ] Modelo `deepseek-coder-v2:16b` baixado
- [ ] Modelfile criado e otimizado
- [ ] Modelo testado com sucesso
- [ ] (Opcional) Quantiza√ß√£o manual executada
- [ ] (Opcional) Quantiza√ß√µes testadas
- [ ] Configura√ß√£o no projeto ANIMA atualizada

---

**√öltima Atualiza√ß√£o**: Janeiro 2025
**Vers√£o**: 1.0
**Status**: Ready for Implementation üöÄ

