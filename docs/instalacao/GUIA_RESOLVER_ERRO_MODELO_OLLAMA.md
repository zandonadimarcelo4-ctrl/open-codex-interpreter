# üîß Guia para Resolver Erro de Modelo LLM N√£o Encontrado no Ollama

## ‚ùå Erro Comum

```
Ollama API error: 404 - model not found
Modelo 'deepseek-coder-v2-16b-q4_k_m' n√£o encontrado no Ollama
```

## üîç Causas Poss√≠veis

1. **Modelo n√£o est√° instalado** - O modelo n√£o foi baixado no Ollama
2. **Nome do modelo incorreto** - O nome do modelo est√° errado ou mudou
3. **Ollama n√£o est√° rodando** - O servidor Ollama n√£o est√° ativo
4. **Vari√°vel de ambiente incorreta** - `DEFAULT_MODEL` aponta para modelo inexistente

## ‚úÖ Solu√ß√µes

### 1. Verificar Modelos Instalados

```bash
# Listar todos os modelos instalados
ollama list

# Ver modelos dispon√≠veis online
ollama search deepseek
```

### 2. Instalar Modelo Padr√£o

```bash
# Instalar modelo padr√£o (DeepSeek-Coder-V2)
ollama pull deepseek-coder-v2-16b-q4_k_m

# Ou instalar modelo mais leve (mais r√°pido)
ollama pull deepseek-coder

# Ou instalar modelo espec√≠fico usado pelo sistema
ollama pull lucasmg/deepseek-r1-8b-0528-qwen3-q4_K_M-tool-true
```

### 3. Configurar Modelo no .env

Edite o arquivo `.env` e defina o modelo que voc√™ tem instalado:

```env
# Modelo padr√£o (use um modelo que voc√™ tem instalado)
DEFAULT_MODEL=deepseek-coder-v2-16b-q4_k_m

# Ou use um modelo mais leve
# DEFAULT_MODEL=deepseek-coder

# Ou use o modelo espec√≠fico do sistema
# DEFAULT_MODEL=lucasmg/deepseek-r1-8b-0528-qwen3-q4_K_M-tool-true
```

### 4. Verificar se Ollama Est√° Rodando

```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Se n√£o estiver rodando, inicie o Ollama
ollama serve
```

### 5. Usar Modelo de Fallback Autom√°tico

O sistema agora tem fallback autom√°tico! Se o modelo configurado n√£o estiver dispon√≠vel, o sistema tentar√° usar modelos alternativos na seguinte ordem:

1. `deepseek-coder-v2-16b-q4_k_m`
2. `deepseek-coder`
3. `deepseek-coder:1.3b`
4. `codellama`
5. `mistral`
6. `llama3.2`
7. `qwen2.5-coder`

## üöÄ Solu√ß√£o R√°pida

### Passo 1: Verificar Modelos Instalados
```bash
ollama list
```

### Passo 2: Instalar Modelo (se necess√°rio)
```bash
# Op√ß√£o 1: Modelo padr√£o (recomendado)
ollama pull deepseek-coder-v2-16b-q4_k_m

# Op√ß√£o 2: Modelo mais leve (mais r√°pido)
ollama pull deepseek-coder

# Op√ß√£o 3: Modelo usado pelo sistema
ollama pull lucasmg/deepseek-r1-8b-0528-qwen3-q4_K_M-tool-true
```

### Passo 3: Configurar .env
```env
DEFAULT_MODEL=deepseek-coder-v2-16b-q4_k_m
```

### Passo 4: Reiniciar Servidor
```bash
# Reinicie o servidor para aplicar as mudan√ßas
npm run dev
```

## üìã Modelos Recomendados

### Para Desenvolvimento (Leves e R√°pidos)
- `deepseek-coder` - Modelo leve e r√°pido
- `codellama` - Modelo especializado em c√≥digo
- `mistral` - Modelo geral r√°pido

### Para Produ√ß√£o (Melhor Qualidade)
- `deepseek-coder-v2-16b-q4_k_m` - Alta qualidade, quantizado
- `lucasmg/deepseek-r1-8b-0528-qwen3-q4_K_M-tool-true` - Modelo com tools

### Para Testes (Muito Leves)
- `deepseek-coder:1.3b` - Muito leve, r√°pido
- `qwen2.5-coder:1.5b` - Muito leve

## üîç Verifica√ß√£o de Status

### Verificar Ollama
```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Deve retornar JSON com lista de modelos
```

### Verificar Modelo Espec√≠fico
```bash
# Verificar se modelo espec√≠fico est√° instalado
ollama list | grep deepseek-coder
```

### Testar Modelo
```bash
# Testar modelo diretamente
ollama run deepseek-coder-v2-16b-q4_k_m "Hello, world!"
```

## ‚ö†Ô∏è Erros Comuns

### Erro: "connection refused"
**Causa**: Ollama n√£o est√° rodando
**Solu√ß√£o**: Execute `ollama serve` ou reinicie o Ollama

### Erro: "model not found"
**Causa**: Modelo n√£o est√° instalado
**Solu√ß√£o**: Execute `ollama pull <nome-do-modelo>`

### Erro: "timeout"
**Causa**: Modelo muito lento ou Ollama sobrecarregado
**Solu√ß√£o**: 
- Use modelo mais leve
- Aumente `OLLAMA_TIMEOUT_MS` no `.env`
- Verifique recursos do sistema (CPU/RAM/GPU)

## üõ†Ô∏è Troubleshooting Avan√ßado

### Listar Modelos Dispon√≠veis Programaticamente
```typescript
import { listAvailableModels } from "./server/utils/ollama";

const models = await listAvailableModels();
console.log("Modelos dispon√≠veis:", models);
```

### Encontrar Melhor Modelo Dispon√≠vel
```typescript
import { findBestAvailableModel } from "./server/utils/ollama";

const model = await findBestAvailableModel();
console.log("Melhor modelo dispon√≠vel:", model);
```

### Verificar Modelo Espec√≠fico
```typescript
import { checkModelAvailable } from "./server/utils/ollama";

const available = await checkModelAvailable("deepseek-coder-v2-16b-q4_k_m");
console.log("Modelo dispon√≠vel:", available);
```

## üìö Refer√™ncias

- [Ollama Documentation](https://ollama.ai/docs)
- [DeepSeek-Coder Models](https://ollama.ai/library/deepseek-coder)
- [Model Installation Guide](./GUIA_DEEPSEEK_CODER_V2_OLLAMA.md)

## üéØ Resumo

1. ‚úÖ Verifique modelos instalados: `ollama list`
2. ‚úÖ Instale modelo se necess√°rio: `ollama pull <modelo>`
3. ‚úÖ Configure `.env` com modelo instalado
4. ‚úÖ Reinicie servidor
5. ‚úÖ Sistema usa fallback autom√°tico se modelo n√£o estiver dispon√≠vel

## üí° Dica

O sistema agora detecta automaticamente se o modelo n√£o est√° dispon√≠vel e tenta usar modelos alternativos. Se nenhum modelo estiver dispon√≠vel, voc√™ ver√° uma mensagem clara com instru√ß√µes de como instalar.

