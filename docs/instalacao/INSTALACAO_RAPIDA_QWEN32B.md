# âš¡ InstalaÃ§Ã£o RÃ¡pida: Qwen2.5-32B-MoE (CÃ©rebro EstratÃ©gico)

## ğŸ¯ Objetivo

Instalar e configurar o **Qwen2.5-32B-Instruct-MoE (Q4_K_M)** como cÃ©rebro estratÃ©gico do sistema.

**Por quÃª?**
- âœ… **Mais inteligente** que todos os outros modelos que cabem em 16GB
- âœ… **RaciocÃ­nio tipo GPT-4-turbo** (offline)
- âœ… **Suporta function calling** nativo
- âœ… **Cabe perfeitamente** em 16GB VRAM (RTX 4080 Super)
- âœ… **Arquitetura MoE**: apenas 2-4 especialistas ativam por token (economia de VRAM)

---

## ğŸš€ InstalaÃ§Ã£o (5 minutos)

### Windows
```bash
# Executar script automÃ¡tico
scripts\setup_qwen32b_moe_rtx.bat
```

### Linux/macOS
```bash
# Instalar modelo base
ollama pull qwen2.5:32b

# Criar modelo customizado
ollama create qwen2.5-32b-instruct-moe-rtx -f Modelfile.qwen2.5-32b-instruct-moe-rtx
```

---

## ğŸ”§ ConfiguraÃ§Ã£o

### 1. Atualizar `.env`
```env
# Modelo cÃ©rebro estratÃ©gico
DEFAULT_MODEL=qwen2.5-32b-instruct-moe-rtx

# Modelo executor (opcional, para cÃ³digo rÃ¡pido)
EXECUTOR_MODEL=deepseek-coder-v2-lite:instruct
```

### 2. Reiniciar o servidor
```bash
# Parar o servidor atual (Ctrl+C)
# Reiniciar o servidor
npm run dev
```

---

## âœ… VerificaÃ§Ã£o

### Verificar se estÃ¡ instalado
```bash
ollama list | grep qwen2.5-32b
```

### Testar o modelo
```bash
ollama run qwen2.5-32b-instruct-moe-rtx "Hello, como vocÃª estÃ¡? Pode me ajudar a planejar uma tarefa complexa?"
```

### Verificar uso de VRAM
```bash
# Windows
nvidia-smi

# Linux
nvidia-smi
```

**VRAM esperada:** ~12-14GB (deixa ~2-4GB livres)

---

## ğŸ“Š ComparaÃ§Ã£o de Modelos

| Modelo | InteligÃªncia | VRAM | Tool Calling | Velocidade | Recomendado para |
|--------|--------------|------|--------------|------------|------------------|
| **qwen2.5-32b-instruct-moe-rtx** | **ğŸ§  148** | **~13GB** | **âœ… Nativo** | **âš™ï¸ MÃ©dia** | **CÃ©rebro estratÃ©gico** |
| qwen2.5:14b | ğŸ§  141 | ~9GB | âœ… Nativo | ğŸš€ RÃ¡pida | Alternativa |
| qwen2.5:7b | ğŸ§  138 | ~4GB | âœ… Nativo | ğŸš€ Muito rÃ¡pida | Testes rÃ¡pidos |
| llama3.2:3b | ğŸ§  135 | ~2GB | âœ… Nativo | ğŸš€ Muito rÃ¡pida | Desenvolvimento |

---

## ğŸ› Troubleshooting

### Erro: "model not found"
```bash
# Instalar modelo base primeiro
ollama pull qwen2.5:32b

# Criar modelo customizado
ollama create qwen2.5-32b-instruct-moe-rtx -f Modelfile.qwen2.5-32b-instruct-moe-rtx
```

### Erro: "out of memory"
```bash
# Verificar VRAM disponÃ­vel
nvidia-smi

# Se nÃ£o tiver 16GB, usar modelo menor
DEFAULT_MODEL=qwen2.5:14b
```

### Erro: "still does not support tools"
```bash
# Verificar versÃ£o do Ollama
ollama --version

# Atualizar Ollama (precisa ser >= 0.1.0)
# Windows: Baixar de https://ollama.ai
# Linux: sudo apt update && sudo apt upgrade ollama
```

### Modelo muito lento
```bash
# Reduzir contexto (mais rÃ¡pido, menos inteligente)
# Editar Modelfile.qwen2.5-32b-instruct-moe-rtx
PARAMETER num_ctx 4096  # Reduzir de 8192 para 4096

# Recriar modelo
ollama create qwen2.5-32b-instruct-moe-rtx -f Modelfile.qwen2.5-32b-instruct-moe-rtx
```

---

## ğŸ¯ Arquitetura HÃ­brida (Recomendada)

### CÃ©rebro EstratÃ©gico: Qwen32B-MoE
- **Uso:** RaciocÃ­nio, planejamento, tool-calling, auto-reflexÃ£o
- **VRAM:** ~12-14GB
- **Velocidade:** MÃ©dia (compensa com inteligÃªncia)

### Executor RÃ¡pido: DeepSeek-Lite
- **Uso:** GeraÃ§Ã£o de cÃ³digo, execuÃ§Ã£o, debugging
- **VRAM:** ~8.5GB (carregado sob demanda)
- **Velocidade:** RÃ¡pida

**Resultado:** Sistema com inteligÃªncia tipo GPT-4-turbo + execuÃ§Ã£o local eficiente.

---

## ğŸ“ Notas

- **MoE (Mixture of Experts):** Apenas 2-4 especialistas ativam por token
- **Economia de VRAM:** Consumo efetivo ~12-14GB (nÃ£o 32GB)
- **Performance:** Similar a GPT-4-turbo em raciocÃ­nio e cÃ³digo
- **Tool calling:** Suporte nativo via JSON/function-calling
- **Velocidade:** MÃ©dia (devido ao tamanho, mas compensa com inteligÃªncia)

---

**Pronto! Agora vocÃª tem o modelo mais inteligente possÃ­vel rodando na sua RTX 4080 Super!** ğŸ‰

