# ðŸš€ InstalaÃ§Ã£o RÃ¡pida: DeepSeek-Coder-V2:16b Q4_K_M para RTX NVIDIA

## ðŸ“‹ VisÃ£o Geral

Guia rÃ¡pido para instalar e configurar o modelo **DeepSeek-Coder-V2:16b** otimizado para **GPU NVIDIA RTX** com quantizaÃ§Ã£o **Q4_K_M**.

---

## âš¡ InstalaÃ§Ã£o RÃ¡pida (Recomendado)

### Windows

```bash
# Executar script de instalaÃ§Ã£o
scripts\setup_deepseek_coder_v2_16b_q4_k_m_rtx.bat
```

### Linux/macOS

```bash
# Tornar script executÃ¡vel
chmod +x scripts/setup_deepseek_coder_v2_16b_q4_k_m_rtx.sh

# Executar script
./scripts/setup_deepseek_coder_v2_16b_q4_k_m_rtx.sh
```

### Python (Multiplataforma)

```bash
# Executar script Python
python scripts/setup_deepseek_coder_v2_16b_q4_k_m_rtx.py
```

---

## ðŸ“ InstalaÃ§Ã£o Manual

### Passo 1: Baixar Modelo

```bash
ollama pull deepseek-coder-v2:16b
```

### Passo 2: Criar Modelo Otimizado

```bash
# Usar Modelfile otimizado para RTX
ollama create deepseek-coder-v2-16b-q4_k_m-rtx -f Modelfile.deepseek-coder-v2-16b-q4_k_m-rtx
```

### Passo 3: Testar Modelo

```bash
ollama run deepseek-coder-v2-16b-q4_k_m-rtx "Write a Python function to calculate fibonacci numbers"
```

---

## ðŸ”§ ConfiguraÃ§Ã£o no Projeto ANIMA

### 1. Atualizar `.env`

```bash
# Adicionar ao arquivo .env
DEFAULT_MODEL=deepseek-coder-v2-16b-q4_k_m-rtx
OLLAMA_BASE_URL=http://localhost:11434
```

### 2. Verificar GPU NVIDIA

```bash
# Verificar se GPU estÃ¡ sendo usada
nvidia-smi

# Durante execuÃ§Ã£o do modelo, vocÃª deve ver uso de GPU
```

### 3. Reiniciar Servidor

```bash
# Reiniciar servidor para aplicar configuraÃ§Ãµes
npm run dev
# ou
python main.py
```

---

## ðŸ“Š EspecificaÃ§Ãµes do Modelo

| EspecificaÃ§Ã£o | Valor |
|---------------|-------|
| **Modelo** | DeepSeek-Coder-V2:16b |
| **QuantizaÃ§Ã£o** | Q4_K_M (otimizado) |
| **Tamanho** | ~8.9GB |
| **Context Window** | 32,768 tokens (otimizado para GPU) |
| **GPU Layers** | 99 (todas na GPU) |
| **Batch Size** | 1024 (otimizado para RTX) |
| **Flash Attention** | Habilitado |
| **VRAM Requerida** | ~6-8GB (dependendo da RTX) |

---

## ðŸŽ¯ OtimizaÃ§Ãµes para RTX NVIDIA

### ConfiguraÃ§Ãµes Aplicadas

1. **GPU Layers: 99** - Todas as camadas na GPU
2. **Batch Size: 1024** - Processamento em lote maior (RTX suporta)
3. **Flash Attention: true** - AtenÃ§Ã£o flash para acelerar
4. **Context Window: 32,768** - Aproveita melhor a GPU
5. **Num Threads: 8** - CPU apenas para tarefas leves

### Verificar Uso de GPU

```bash
# Monitorar uso de GPU em tempo real
watch -n 1 nvidia-smi

# Ou no Windows
nvidia-smi -l 1
```

---

## ðŸ› Troubleshooting

### Erro: "CUDA out of memory"

```bash
# Reduzir batch size no Modelfile
PARAMETER num_batch 512  # Reduzir de 1024 para 512

# Ou reduzir context window
PARAMETER num_ctx 16384  # Reduzir de 32768 para 16384
```

### Erro: "Model not found"

```bash
# Verificar se modelo estÃ¡ instalado
ollama list

# Reinstalar modelo
ollama pull deepseek-coder-v2:16b
ollama create deepseek-coder-v2-16b-q4_k_m-rtx -f Modelfile.deepseek-coder-v2-16b-q4_k_m-rtx
```

### GPU nÃ£o estÃ¡ sendo usada

```bash
# Verificar se Ollama detecta GPU
ollama ps

# Verificar drivers NVIDIA
nvidia-smi

# Reiniciar Ollama
# Windows: Reiniciar serviÃ§o Ollama
# Linux: sudo systemctl restart ollama
```

### Performance lenta

```bash
# Verificar se Flash Attention estÃ¡ habilitado
# Verificar uso de GPU (deve estar >80%)
nvidia-smi

# Verificar se batch size estÃ¡ adequado
# RTX 3060-3070: 512
# RTX 3080-3090: 1024
# RTX 4080-4090: 2048
```

---

## ðŸ“š ReferÃªncias

- [Ollama DeepSeek-Coder-V2](https://ollama.com/library/deepseek-coder-v2)
- [DeepSeek-Coder-V2 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)
- [Ollama Modelfile Documentation](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

---

## âœ… Checklist

- [ ] Ollama instalado
- [ ] GPU NVIDIA RTX detectada
- [ ] Modelo `deepseek-coder-v2:16b` baixado
- [ ] Modelo otimizado `deepseek-coder-v2-16b-q4_k_m-rtx` criado
- [ ] Modelo testado com sucesso
- [ ] GPU sendo usada (verificar com `nvidia-smi`)
- [ ] ConfiguraÃ§Ã£o no `.env` atualizada
- [ ] Servidor reiniciado

---

## ðŸŽ‰ Pronto!

Agora o modelo estÃ¡ configurado e otimizado para usar sua GPU NVIDIA RTX!

Para usar no projeto ANIMA, o sistema irÃ¡ automaticamente:
1. Detectar o modelo `deepseek-coder-v2-16b-q4_k_m-rtx`
2. Usar GPU NVIDIA se disponÃ­vel
3. Fazer fallback para CPU se necessÃ¡rio
4. Otimizar performance para cÃ³digo

---

**Ãšltima AtualizaÃ§Ã£o**: Janeiro 2025
**VersÃ£o**: 1.0
**Status**: Ready for RTX NVIDIA ðŸš€

