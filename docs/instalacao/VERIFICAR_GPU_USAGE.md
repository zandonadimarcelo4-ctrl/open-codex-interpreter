# ğŸ” Como Verificar se o Modelo estÃ¡ Usando GPU

## ğŸ“Š VerificaÃ§Ã£o RÃ¡pida

### 1. Verificar Uso de GPU Durante ExecuÃ§Ã£o

```bash
# Em um terminal, execute o modelo
ollama run deepseek-coder-v2-16b-q4_k_m-rtx "Write a Python function"

# Em outro terminal, monitore a GPU
nvidia-smi -l 1
```

**O que procurar:**
- **Uso de GPU**: Deve estar entre 80-100% durante inferÃªncia
- **VRAM Usada**: Deve estar entre 10-12GB (de 16GB)
- **Temperatura**: 60-75Â°C
- **Power Usage**: Alta (300-400W durante inferÃªncia)

### 2. Verificar VariÃ¡veis de Ambiente

```bash
# Verificar se CUDA estÃ¡ disponÃ­vel
echo %CUDA_VISIBLE_DEVICES%

# Verificar se Ollama detecta GPU
ollama ps
```

### 3. Testar Performance

```bash
# Testar velocidade de inferÃªncia
ollama run deepseek-coder-v2-16b-q4_k_m-rtx "Write a Python function to calculate fibonacci numbers" --verbose
```

**Performance Esperada (RTX 4080 Super 16GB):**
- **Tokens/segundo**: 80-120 tokens/s (com GPU)
- **Tokens/segundo**: 10-20 tokens/s (sem GPU, apenas CPU)
- **Tempo de carregamento**: 5-10 segundos (com GPU)
- **Tempo de carregamento**: 30-60 segundos (sem GPU)

### 4. Verificar Logs do Ollama

```bash
# Verificar logs do Ollama (Windows)
# Os logs geralmente mostram se estÃ¡ usando GPU ou CPU
```

## ğŸ› Problemas Comuns

### GPU nÃ£o estÃ¡ sendo usada

**Sintomas:**
- Velocidade baixa (10-20 tokens/s)
- Uso de GPU baixo (<10%)
- Tempo de carregamento longo (>30 segundos)

**SoluÃ§Ãµes:**

1. **Verificar Drivers NVIDIA**:
   ```bash
   nvidia-smi
   # Deve mostrar informaÃ§Ãµes da GPU
   ```

2. **Verificar CUDA**:
   ```bash
   nvcc --version
   # Deve mostrar versÃ£o do CUDA
   ```

3. **Reiniciar Ollama**:
   ```bash
   # Windows: Reiniciar serviÃ§o Ollama
   # Ou reiniciar o computador
   ```

4. **Verificar VariÃ¡veis de Ambiente**:
   ```bash
   # NÃ£o definir CUDA_VISIBLE_DEVICES (deixe vazio)
   # Ollama deve detectar GPU automaticamente
   ```

### Performance baixa mesmo com GPU

**PossÃ­veis causas:**
- Context window muito grande (reduzir `num_ctx`)
- Batch size muito grande (Ollama gerencia automaticamente)
- Outros processos usando GPU

**SoluÃ§Ãµes:**
- Fechar outros aplicativos que usam GPU
- Reduzir `num_ctx` se nÃ£o precisar de contexto muito grande
- Verificar temperatura da GPU (throttling)

## âœ… Checklist

- [ ] GPU detectada pelo `nvidia-smi`
- [ ] Drivers NVIDIA atualizados
- [ ] CUDA instalado e funcionando
- [ ] Ollama detecta GPU (verificar logs)
- [ ] Uso de GPU >80% durante inferÃªncia
- [ ] VRAM sendo usada (10-12GB)
- [ ] Performance adequada (80-120 tokens/s)
- [ ] Temperatura normal (60-75Â°C)

## ğŸ“š ReferÃªncias

- [Ollama GPU Support](https://github.com/ollama/ollama/blob/main/docs/gpu.md)
- [NVIDIA CUDA Documentation](https://docs.nvidia.com/cuda/)
- [Ollama Performance Tuning](https://github.com/ollama/ollama/blob/main/docs/performance.md)

