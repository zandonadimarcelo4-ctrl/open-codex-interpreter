# Classificador de Inten√ß√£o com Mistral 7B Instruct

## üìã Resumo

O classificador de inten√ß√£o foi atualizado para usar o modelo **Mistral 7B Instruct** como modelo padr√£o recomendado. Este modelo √© leve (~4.4 GB), r√°pido e excelente em seguir instru√ß√µes JSON estruturado.

## üéØ Modelo Selecionado

- **Modelo**: `mistral:7b-instruct`
- **Tamanho**: ~4.4 GB VRAM
- **Caracter√≠sticas**:
  - Leve e r√°pido
  - Excelente em seguir instru√ß√µes JSON
  - Boa capacidade de classifica√ß√£o de inten√ß√£o
  - Baixa lat√™ncia para classifica√ß√£o

## üîß Configura√ß√£o

### 1. Instalar o Modelo

```bash
ollama pull mistral:7b-instruct
```

Ou use o script automatizado:

```bash
scripts/install_intent_classifier_model.bat
```

### 2. Configurar Vari√°vel de Ambiente

Adicione ao arquivo `.env`:

```env
INTENT_CLASSIFIER_MODEL=mistral:7b-instruct
```

### 3. Verificar Instala√ß√£o

Teste o classificador:

```bash
python interpreter/intent_classifier.py "Ol√°, como voc√™ est√°?"
```

## üìä Modelos de Fallback

O sistema possui uma ordem de fallback inteligente:

1. **mistral:7b-instruct** (RECOMENDADO) - 4.4 GB
2. **phi3:mini** (Mais r√°pido) - 2.2 GB
3. **qwen2.5-coder:7b** - 4.7 GB
4. **qwen2.5:7b-instruct** - 4.7 GB
5. **llama3.1:8b** - 4.9 GB
6. **qwen2.5-coder:7b-instruct** - 4.7 GB
7. **llama3.2:3b** (se dispon√≠vel)
8. **deepseek-coder:6.7b** (se dispon√≠vel)
9. **DEFAULT_MODEL** (√∫ltimo recurso, pode ser grande)

## üöÄ Uso

### Classifica√ß√£o H√≠brida (Recomendado)

```python
from interpreter.intent_classifier import classify_intent_hybrid

result = classify_intent_hybrid("Crie um script Python para fazer backup")
# {
#   "intent": "execution",
#   "reasoning": "O usu√°rio pediu para criar um script Python.",
#   "action_type": "code",
#   "confidence": 0.95
# }
```

### Classifica√ß√£o LLM Direta

```python
from interpreter.intent_classifier import classify_intent_llm

result = classify_intent_llm("Ol√°, como voc√™ est√°?", model="mistral:7b-instruct")
# {
#   "intent": "conversation",
#   "reasoning": "√â uma sauda√ß√£o.",
#   "action_type": null,
#   "confidence": 0.95
# }
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Timeout

O timeout padr√£o √© de **60 segundos** para modelos que podem demorar mais:

```python
# Em interpreter/intent_classifier.py
timeout=60  # Timeout aumentado para modelos que podem demorar mais
```

### Temperatura

A temperatura √© configurada para **0.1** para classifica√ß√£o mais consistente:

```python
"options": {
    "temperature": 0.1,  # Baixa temperatura para classifica√ß√£o mais consistente
    "num_predict": 200,  # Resposta curta (apenas JSON)
}
```

## üîç Integra√ß√£o

### TypeScript Bridge

O classificador √© acess√≠vel via TypeScript atrav√©s do bridge:

```typescript
import { classifyIntentHybrid } from "./utils/intent_classifier_bridge";

const result = await classifyIntentHybrid("Ol√°, como voc√™ est√°?");
```

### AutoGen Router

O classificador √© usado automaticamente no router do AutoGen:

```typescript
// Em autogen_agent_interface/server/routers.ts
const { classifyIntentHybrid } = await import("./utils/intent_classifier_bridge");
const llmIntent = await classifyIntentHybrid(input.message, rulesIntent);
```

## üìù Formato de Resposta

O classificador retorna um objeto JSON com a seguinte estrutura:

```typescript
interface IntentClassification {
  intent: "execution" | "conversation";
  reasoning: string;
  action_type: "code" | "web" | "file" | "search" | "general" | null;
  confidence: number; // 0.0 - 1.0
}
```

### Exemplos

**Conversa:**
```json
{
  "intent": "conversation",
  "reasoning": "√â uma sauda√ß√£o.",
  "action_type": null,
  "confidence": 0.95
}
```

**Execu√ß√£o (C√≥digo):**
```json
{
  "intent": "execution",
  "reasoning": "O usu√°rio pediu para criar um script Python.",
  "action_type": "code",
  "confidence": 0.95
}
```

**Execu√ß√£o (Web):**
```json
{
  "intent": "execution",
  "reasoning": "O usu√°rio pediu para pesquisar na web.",
  "action_type": "web",
  "confidence": 0.95
}
```

## üêõ Troubleshooting

### Modelo n√£o encontrado

Se o modelo `mistral:7b-instruct` n√£o estiver dispon√≠vel, o sistema tentar√° automaticamente os modelos de fallback na ordem listada acima.

### Timeout

Se estiver ocorrendo timeout, verifique:
1. Se o Ollama est√° rodando: `ollama serve`
2. Se h√° VRAM suficiente dispon√≠vel
3. Se a conex√£o com o Ollama est√° funcionando

### Erro de JSON

Se houver erro ao parsear JSON, o sistema tentar√° extrair o JSON da resposta usando regex e markdown code blocks.

## üìö Refer√™ncias

- [Mistral AI](https://mistral.ai/)
- [Ollama Models](https://ollama.com/library/mistral)
- [Intent Classifier Documentation](../interpreter/intent_classifier.py)

