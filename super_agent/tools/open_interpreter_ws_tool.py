"""
Open Interpreter WebSocket Tool para AutoGen v2
Tool que envia comandos ao Open Interpreter via WebSocket
O Open Interpreter pensa e executa localmente usando seu modelo interno (Ollama)
AutoGen comanda QUANDO e PORQU√ä; Open Interpreter decide COMO
"""
import os
import json
import asyncio
import logging
from typing import Dict, Any, Optional
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente
load_dotenv()

logger = logging.getLogger(__name__)

# Configura√ß√µes
OI_WS_URL = os.getenv("OI_WS_URL", "ws://localhost:4000")
WORKDIR = os.getenv("WORKDIR", os.getcwd())

try:
    import websockets
    from websockets.exceptions import ConnectionClosed
    WEBSOCKETS_AVAILABLE = True
except ImportError:
    WEBSOCKETS_AVAILABLE = False
    logger.warning("websockets n√£o dispon√≠vel. Use: pip install websockets")

# Contrato do sistema para o Open Interpreter (subordinado ao AutoGen)
SYSTEM_CONTRACT = f"""
You are the Open Interpreter subordinate agent.

- Obey Commander (AutoGen) precisely.
- Stay inside WORKDIR: {WORKDIR}
- For destructive actions (delete/move/overwrite):
  1) Print a clear plan.
  2) Wait for a follow-up message containing ONLY: CONFIRM
- Always show the code before executing.
- Be concise with logs and end with a summary.
- Think and execute locally using your internal LLM.
- Generate code, debug, and execute autonomously within the command scope.
"""


async def _send_to_oi(
    order: str,
    temperature: float = 0.2,
    max_tokens: int = 2048,
    workdir: Optional[str] = None,
) -> str:
    """
    Envia comando ao Open Interpreter via WebSocket.
    
    O Open Interpreter pensa e executa localmente usando seu modelo interno.
    
    Args:
        order: Comando/tarefa em linguagem natural
        temperature: Temperatura para o modelo (opcional)
        max_tokens: M√°ximo de tokens (opcional)
        workdir: Diret√≥rio de trabalho (opcional, usa WORKDIR se n√£o fornecido)
    
    Returns:
        Output do Open Interpreter (pensamento + execu√ß√£o)
    """
    if not WEBSOCKETS_AVAILABLE:
        raise ImportError("websockets n√£o est√° dispon√≠vel. Instale: pip install websockets")
    
    # Preparar payload
    payload = {
        "type": "prompt",  # Tipo: prompt (comando em linguagem natural)
        "prompt": f"{SYSTEM_CONTRACT}\n\n[COMMANDER ORDER]\n{order}",
        "temperature": temperature,
        "max_tokens": max_tokens,
        "workdir": workdir or WORKDIR,
    }
    
    output = ""
    response_text = ""
    code_executed = ""
    
    try:
        async with websockets.connect(OI_WS_URL) as ws:
            # Enviar comando
            await ws.send(json.dumps(payload))
            
            # Receber respostas (pode vir em m√∫ltiplas mensagens)
            async for msg in ws:
                try:
                    data = json.loads(msg)
                    
                    if data.get("type") == "status":
                        status_msg = data.get("message", "")
                        logger.info(f"üì° Open Interpreter: {status_msg}")
                        print(f"üí¨ {status_msg}")
                    
                    elif data.get("type") == "response":
                        # Resposta completa do Open Interpreter
                        response_text = data.get("response", "")
                        output = data.get("output", "")
                        code_executed = data.get("code_executed", "")
                        
                        # Exibir output se houver
                        if output:
                            logger.info(f"üìä Output: {output}")
                            print(f"üìä Output:\n{output}")
                    
                    elif data.get("type") == "done":
                        # Conclu√≠do
                        break
                    
                    elif data.get("type") == "error":
                        error = data.get("error", "Erro desconhecido")
                        logger.error(f"‚ùå Erro: {error}")
                        raise Exception(f"Open Interpreter error: {error}")
                
                except json.JSONDecodeError:
                    logger.warning(f"Mensagem inv√°lida recebida: {msg}")
                    continue
        
        # Combinar resposta e output
        if output:
            return f"‚úÖ Open Interpreter executou:\n\nüí≠ Pensamento: {response_text}\n\nüìä Output:\n{output}"
        else:
            return f"‚úÖ Open Interpreter respondeu:\n\nüí≠ {response_text}"
    
    except ConnectionClosed:
        error_msg = f"Conex√£o WebSocket fechada. Verifique se o servidor Open Interpreter est√° rodando em {OI_WS_URL}"
        logger.error(error_msg)
        raise Exception(error_msg)
    except Exception as e:
        error_msg = f"Erro ao comunicar com Open Interpreter: {e}"
        logger.error(error_msg)
        raise Exception(error_msg)


def register_open_interpreter_tool():
    """
    Registra a tool do Open Interpreter para AutoGen v2.
    
    Returns:
        Lista de tools para AutoGen v2
    """
    if not WEBSOCKETS_AVAILABLE:
        logger.warning("websockets n√£o dispon√≠vel. Tool do Open Interpreter n√£o ser√° registrada.")
        return []
    
    return [{
        "name": "open_interpreter_agent",
        "description": (
            "Envia comandos/tarefas ao Open Interpreter que pensa e executa localmente. "
            "O Open Interpreter usa seu modelo interno (Ollama) para raciocinar, gerar c√≥digo e executar. "
            "Use esta tool quando precisar que o Interpreter: "
            "- Raciocine sobre uma tarefa complexa "
            "- Gere e execute c√≥digo automaticamente "
            "- Corrija erros e tente novamente "
            "- Execute scripts, comandos shell, etc. "
            "O AutoGen comanda o QUANDO e o PORQU√ä; o Open Interpreter decide o COMO e executa."
        ),
        "func": lambda command: asyncio.run(_send_to_oi(command))
    }]


# Fun√ß√£o para obter schema da tool (compat√≠vel com AutoGen v2)
def get_open_interpreter_tool_schema() -> Dict[str, Any]:
    """
    Retorna o schema da tool para AutoGen v2.
    """
    return {
        "type": "function",
        "function": {
            "name": "open_interpreter_agent",
            "description": (
                "Envia comandos/tarefas ao Open Interpreter que pensa e executa localmente. "
                "O Open Interpreter usa seu modelo interno (Ollama) para raciocinar, gerar c√≥digo e executar. "
                "O AutoGen comanda o QUANDO e o PORQU√ä; o Open Interpreter decide o COMO e executa."
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": (
                            "Comando/tarefa em linguagem natural para o Open Interpreter. "
                            "Exemplos: 'Crie um script Python que soma 5 + 7', "
                            "'Execute ls -la no diret√≥rio atual', "
                            "'Analise o arquivo data.csv e gere um relat√≥rio'"
                        ),
                    },
                },
                "required": ["command"],
            },
        },
    }

